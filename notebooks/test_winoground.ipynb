{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST USING OSCAR CHECKPOINT\n",
    "\n",
    "# load examples with object features (features.tsv file)\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"/fsx/harman/Oscar\")\n",
    "from oscar.modeling.modeling_bert import BertImgForPreTraining\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertTokenizer)\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def get_img_feats(data_dir, file_name='features.tsv'):\n",
    "    img_feats_df = pd.read_csv(os.path.join(data_dir, file_name),sep='\\t',header=None,converters={1:ast.literal_eval},index_col=0)\n",
    "    img_feat_dict = {}\n",
    "    for i in range(img_feats_df.shape[0]):\n",
    "        num_boxes = img_feats_df.iloc[i][1]\n",
    "        \n",
    "        features = img_feats_df.iloc[i][2]\n",
    "        img_feat_dict[img_feats_df.iloc[i].name] = {'num_boxes':num_boxes, 'features':features}\n",
    "        \n",
    "    \n",
    "    return img_feat_dict\n",
    "\n",
    "def get_img_predictions(data_dir, file_name='predictions.tsv'):\n",
    "    img_predictions_df = pd.read_csv(os.path.join(data_dir, file_name),sep='\\t',header = None,converters={1:json.loads},index_col=0)#converters={1:ast.literal_eval})\n",
    "    img_predictions_dict = {}\n",
    "    # print(img_predictions_df.iloc[0][1]['objects'])\n",
    "    for i in range(img_predictions_df.shape[0]):\n",
    "        objectinfo = img_predictions_df.iloc[i][1]['objects']\n",
    "        img_predictions_dict[img_predictions_df.iloc[i].name] = objectinfo\n",
    "        \n",
    "    \n",
    "    return img_predictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_img_predictions(winoroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/fsx/harman/data/VinVL_img_features/winoground\"\n",
    "# file_name = \"winoground_test.feature.tsv\"\n",
    "# img_feats_df = pd.read_csv(os.path.join(data_dir, file_name),sep='\\t',header=None,converters={1:ast.literal_eval},index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_feats_df.iloc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "winoroot = \"/fsx/harman/data/VinVL_img_features/winoground\"\n",
    "img_feats = get_img_feats(winoroot, file_name = 'winoground_test.feature.tsv') # dict of the form {imgname: {'num_boxes': 37,'features': 'AAAAAAAAAAAAAAAA}}\n",
    "img_preds = get_img_predictions(winoroot, file_name = 'predictions.tsv') # dict of the form {imgname: [{'rect': [0.0, 280.7784118652344, 1127.890380859375, 1326.784912109375], 'bbox_id': 0, 'class': 'man', 'conf': 0.9323487877845764, 'feature': '+rRRPwAA}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_preds['ex_38_img_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgkeys = list(img_feats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_feats[imgkeys[0]]\n",
    "# imgkeys[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the raw json file of winoground\n",
    "winoground_raw = []\n",
    "with open('/fsx/harman/data/raw_data/winoground/examples.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    \n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    winoground_raw.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winoground_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelpath = /checkpoints/harman/oscar/87/checkpoint-0240000\n",
      "use_prob = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /checkpoints/harman/oscar/87/checkpoint-0240000 were not used when initializing BertImgForPreTraining: ['cls.obj_relation.weight', 'cls.obj_relation.bias']\n",
      "- This IS expected if you are initializing BertImgForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertImgForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:18<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrastive text score: 0.23\n",
      "contrastive image score: 0.0725\n",
      "contrastive group score: 0.0525\n",
      "modelpath = /checkpoints/harman/oscar/87/checkpoint-0240000\n",
      "use_prob = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /checkpoints/harman/oscar/87/checkpoint-0240000 were not used when initializing BertImgForPreTraining: ['cls.obj_relation.weight', 'cls.obj_relation.bias']\n",
      "- This IS expected if you are initializing BertImgForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertImgForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:18<00:00, 21.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrastive text score: 0.235\n",
      "contrastive image score: 0.065\n",
      "contrastive group score: 0.0475\n",
      "modelpath = /checkpoints/harman/oscar/88/checkpoint-0240000\n",
      "use_prob = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:18<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrastive text score: 0.2775\n",
      "contrastive image score: 0.0925\n",
      "contrastive group score: 0.045\n",
      "modelpath = /checkpoints/harman/oscar/88/checkpoint-0240000\n",
      "use_prob = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:18<00:00, 21.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrastive text score: 0.28\n",
      "contrastive image score: 0.0975\n",
      "contrastive group score: 0.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## load oscar model\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertImgForPreTraining, BertTokenizer),\n",
    "}\n",
    "\n",
    "# modelpath = '/fsx/harman/Oscar/pretrained_models/vqa/base/checkpoint-2000000'\n",
    "modelpath = '/fsx/harman/Oscar/pretrained_models/pretrained_base/checkpoint-2000000'\n",
    "# modelpath = \"/checkpoints/harman/oscar/88/checkpoint-0240000\"\n",
    "# modelpath = '/checkpoints/harman/oscar/87/checkpoint-0240000'\n",
    "# modelpath = '/checkpoints/harman/oscar/120/checkpoint-0300000'\n",
    "# modelpath = '/checkpoints/harman/oscar/121/checkpoint-0300000'\n",
    ",\n",
    "# model_paths = ['/fsx/harman/Oscar/pretrained_models/pretrained_base/checkpoint-2000000', '/checkpoints/harman/oscar/120/checkpoint-0300000', '/checkpoints/harman/oscar/121/checkpoint-0300000']\n",
    "model_paths = ['/checkpoints/harman/oscar/87/checkpoint-0240000', '/checkpoints/harman/oscar/88/checkpoint-0240000']\n",
    "\n",
    "use_probs = [0, 2]\n",
    "# use_prob = 0\n",
    "# use_prob = 2\n",
    "\n",
    "for modelpath in model_paths:\n",
    "    for use_prob in use_probs:\n",
    "        print(f'modelpath = {modelpath}')\n",
    "        print(f'use_prob = {use_prob}')\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "        config = config_class.from_pretrained(\n",
    "                modelpath,\n",
    "            )\n",
    "\n",
    "        config.img_layer_norm_eps = 1e-12\n",
    "        config.use_img_layernorm = 1\n",
    "        config.img_feature_dim = 2054\n",
    "        config.img_feature_type = \"faster_r-cnn\"\n",
    "        config.hidden_dropout_prob = 0.3\n",
    "        config.num_contrast_classes = 3\n",
    "        config.output_hidden_states = True\n",
    "        config.obj_relation_vocab_size = 51\n",
    "        config.use_sg = False\n",
    "\n",
    "        model = BertImgForPreTraining.from_pretrained(\n",
    "                modelpath,\n",
    "                config=config, ignore_mismatched_sizes=True)\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        # model.eva0()\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(\n",
    "                modelpath,\n",
    "                do_lower_case=True)\n",
    "\n",
    "        # winoground_raw[0]\n",
    "\n",
    "        def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "            \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "            # This is a simple heuristic which will always truncate the longer sequence\n",
    "            # one token at a time. This makes more sense than truncating an equal percent\n",
    "            # of tokens from each, since if one sequence is very short then each token\n",
    "            # that's truncated likely contains more information than a longer sequence.\n",
    "            while True:\n",
    "                total_length = len(tokens_a) + len(tokens_b)\n",
    "                if total_length <= max_length:\n",
    "                    break\n",
    "                if len(tokens_a) > len(tokens_b):\n",
    "                    tokens_a.pop()\n",
    "                else:\n",
    "                    tokens_b.pop()\n",
    "\n",
    "        def oscar_processor(texta, textb, img_feat, max_seq_length=35, max_img_seq_length=50):\n",
    "            tokens_a = tokenizer.tokenize(texta)\n",
    "            tokens_b = tokenizer.tokenize(textb)\n",
    "\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "            assert len(tokens_a) + len(tokens_b) <= max_seq_length\n",
    "\n",
    "\n",
    "            tokens = []\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in tokens_a:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            assert len(tokens_b) > 0\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "\n",
    "            if img_feat.shape[0] >= max_img_seq_length:\n",
    "                    img_feat = img_feat[0:max_img_seq_length, ] # truncating the img features to be 50 length max!\n",
    "                    img_feat_len = img_feat.shape[0]\n",
    "            else:\n",
    "                img_feat_len = img_feat.shape[0]\n",
    "                padding_matrix = torch.zeros((max_img_seq_length - img_feat.shape[0], img_feat.shape[1]))\n",
    "                img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
    "\n",
    "            if max_img_seq_length > 0:\n",
    "                if img_feat_len > max_img_seq_length:\n",
    "                    input_mask = input_mask + [1] * img_feat_len\n",
    "                else:\n",
    "                    input_mask = input_mask + [1] * img_feat_len\n",
    "                    pad_img_feat_len = max_img_seq_length - img_feat_len\n",
    "                    input_mask = input_mask + ([0] * pad_img_feat_len)\n",
    "\n",
    "            images = torch.stack([img_feat]).to(device = torch.device(\"cuda\"), non_blocking=True)\n",
    "            input_ids = torch.stack([torch.tensor(input_ids, dtype=torch.long)]).to(device = torch.device(\"cuda\"), non_blocking=True)\n",
    "            input_mask = torch.stack([torch.tensor(input_mask, dtype=torch.long)]).to(device = torch.device(\"cuda\"), non_blocking=True)\n",
    "            segment_ids = torch.stack([torch.tensor(segment_ids, dtype=torch.long)]).to(device = torch.device(\"cuda\"), non_blocking=True)\n",
    "\n",
    "            # print(images.shape, input_ids.shape, input_mask.shape, segment_ids.shape)\n",
    "            return input_ids, segment_ids, input_mask, images\n",
    "\n",
    "\n",
    "        # get scores using the contrastive thing, see how that is calculated in the bert model of oscar\n",
    "        winoground_oscar_contrastive_scores = []\n",
    "        img_feature_dim = 2054\n",
    "        for example in tqdm(winoground_raw):\n",
    "            img0objtags = ' '.join([cur_d['class'] for cur_d in img_preds[example['image_0']]])\n",
    "            img1objtags = ' '.join([cur_d['class'] for cur_d in img_preds[example['image_1']]])\n",
    "            img0featstr, img0numobj = img_feats[example['image_0']]['features'], img_feats[example['image_0']]['num_boxes'] \n",
    "            img1featstr, img1numobj = img_feats[example['image_1']]['features'], img_feats[example['image_1']]['num_boxes'] \n",
    "            img0feat = np.frombuffer(base64.b64decode(img0featstr), dtype=np.float32).reshape((img0numobj, img_feature_dim))\n",
    "            img1feat = np.frombuffer(base64.b64decode(img1featstr), dtype=np.float32).reshape((img1numobj, img_feature_dim))\n",
    "            img0feat = torch.from_numpy(np.array(img0feat))\n",
    "            img1feat = torch.from_numpy(np.array(img1feat))\n",
    "\n",
    "\n",
    "\n",
    "            inputs_c0_i0 = oscar_processor(texta=example[\"caption_0\"], textb=img0objtags, img_feat=img0feat)\n",
    "            inputs_c1_i0 = oscar_processor(texta=example[\"caption_1\"], textb=img0objtags, img_feat=img0feat)\n",
    "            inputs_c0_i1 = oscar_processor(texta=example[\"caption_0\"], textb=img1objtags, img_feat=img1feat)\n",
    "            inputs_c1_i1 = oscar_processor(texta=example[\"caption_1\"], textb=img1objtags, img_feat=img1feat)\n",
    "\n",
    "            outputs_c0_i0 = model(inputs_c0_i0[0], inputs_c0_i0[1], inputs_c0_i0[2], img_feats=inputs_c0_i0[3])\n",
    "            outputs_c1_i0 = model(inputs_c1_i0[0], inputs_c1_i0[1], inputs_c1_i0[2], img_feats=inputs_c1_i0[3])\n",
    "            outputs_c0_i1 = model(inputs_c0_i1[0], inputs_c0_i1[1], inputs_c0_i1[2], img_feats=inputs_c0_i1[3])\n",
    "            outputs_c1_i1 = model(inputs_c1_i1[0], inputs_c1_i1[1], inputs_c1_i1[2], img_feats=inputs_c1_i1[3])\n",
    "\n",
    "            # print(outputs_c0_i0)\n",
    "\n",
    "            # oscar_contrastive_scores_c0_i0 = outputs_c0_i0.contrastive_logits_per_image.item()\n",
    "            # oscar_contrastive_scores_c1_i0 = outputs_c1_i0.contrastive_logits_per_image.item()\n",
    "            # oscar_contrastive_scores_c0_i1 = outputs_c0_i1.contrastive_logits_per_image.item()\n",
    "            # oscar_contrastive_scores_c1_i1 = outputs_c1_i1.contrastive_logits_per_image.item()\n",
    "            # winoground_oscar_contrastive_scores.append({\"id\" : example[\"id\"], \"c0_i0\": oscar_contrastive_scores_c0_i0, \"c0_i1\": oscar_contrastive_scores_c0_i1, \"c1_i0\": oscar_contrastive_scores_c1_i0, \"c1_i1\": oscar_contrastive_scores_c1_i1})\n",
    "\n",
    "            # print(outputs_c0_i0[1].view(3)[0].item())\n",
    "            oscar_contrastive_scores_c0_i0 = outputs_c0_i0[1].view(3)[use_prob].item()\n",
    "            oscar_contrastive_scores_c1_i0 = outputs_c1_i0[1].view(3)[use_prob].item()\n",
    "            oscar_contrastive_scores_c0_i1 = outputs_c0_i1[1].view(3)[use_prob].item()\n",
    "            oscar_contrastive_scores_c1_i1 = outputs_c1_i1[1].view(3)[use_prob].item()\n",
    "            winoground_oscar_contrastive_scores.append({\"id\" : example[\"id\"], \"c0_i0\": oscar_contrastive_scores_c0_i0, \"c0_i1\": oscar_contrastive_scores_c0_i1, \"c1_i0\": oscar_contrastive_scores_c1_i0, \"c1_i1\": oscar_contrastive_scores_c1_i1})\n",
    "\n",
    "        # calculate scores\n",
    "        if use_prob == 2:\n",
    "            def text_correct(result):\n",
    "                return result[\"c0_i0\"] < result[\"c1_i0\"] and result[\"c1_i1\"] < result[\"c0_i1\"]\n",
    "\n",
    "            def image_correct(result):\n",
    "                return result[\"c0_i0\"] < result[\"c0_i1\"] and result[\"c1_i1\"] < result[\"c1_i0\"]\n",
    "\n",
    "            def group_correct(result):\n",
    "                return image_correct(result) and text_correct(result)\n",
    "\n",
    "        elif use_prob ==0:\n",
    "            def text_correct(result):\n",
    "                return result[\"c0_i0\"] > result[\"c1_i0\"] and result[\"c1_i1\"] > result[\"c0_i1\"]\n",
    "\n",
    "            def image_correct(result):\n",
    "                return result[\"c0_i0\"] > result[\"c0_i1\"] and result[\"c1_i1\"] > result[\"c1_i0\"]\n",
    "\n",
    "            def group_correct(result):\n",
    "                return image_correct(result) and text_correct(result)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        contrastive_text_correct_count = 0\n",
    "        contrastive_image_correct_count = 0\n",
    "        contrastive_group_correct_count = 0\n",
    "        for result in winoground_oscar_contrastive_scores:\n",
    "          contrastive_text_correct_count += 1 if text_correct(result) else 0\n",
    "          contrastive_image_correct_count += 1 if image_correct(result) else 0\n",
    "          contrastive_group_correct_count += 1 if group_correct(result) else 0\n",
    "\n",
    "        denominator = len(winoground_oscar_contrastive_scores)\n",
    "        print(\"contrastive text score:\", contrastive_text_correct_count/denominator)\n",
    "        print(\"contrastive image score:\", contrastive_image_correct_count/denominator)\n",
    "        print(\"contrastive group score:\", contrastive_group_correct_count/denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6568a17d87f84a7acd9e73b508e76dd07f64729bbd386ca0c13538d039834b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
